{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDbi6PDS9MYO"
   },
   "source": [
    "***Participantes (RM - NOME):***<br>\n",
    "xxxx - xxxxx<br>\n",
    "xxxx - xxxxx<br>\n",
    "xxxx - xxxxx<br>\n",
    "xxxx - xxxxx<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xw6WhaNo4k3"
   },
   "source": [
    "###**Criar um classificador de sentimento aplicando técnicas de PLN**\n",
    "---\n",
    "\n",
    "Utilizando o dataset de revisões de filmes em português [1], criar um classificador de sentimentos que consiga um score na métrica F1 Score superior a 70%.\n",
    "\n",
    "Devem utilizar uma amostra de 20% e randon_state igual a 42 para testar as implementações e mensurar a métrica F1 Score (usar o parâmetro average = 'weighted') o restante dos dados devem ser utilizados para o treinamento (80%).\n",
    "\n",
    "Fique a vontade para testar os métodos de pré-processamento, abordagens, algoritmos e bibliotecas, mas explique e justifique suas decisões.\n",
    "O trabalho poderá ser feito em grupo de até 4 pessoas (mesmo grupo do Startup One).\n",
    "\n",
    "Separe a implementação do seu modelo campeão junto com a parte de validação/teste de forma que o professor consiga executar todo o pipeline do modelo campeão.\n",
    "\n",
    "Composição da nota:\n",
    "- 50% - Demonstrações das aplicações das técnicas de PLN (regras, pré-processamentos, tratamentos, variedade de modelos aplicados, etc.)\n",
    "- 50% - Baseado na performance obtida com o dataset de teste (conforme recomendação da amostra) no seu modelo campeão e na validação que o professor processar (Métrica F1 Score)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzhQpodBpRpX"
   },
   "source": [
    "[1] - https://dados-ml-pln.s3-sa-east-1.amazonaws.com/reviews-pt-br.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FyKC9Vhkp0BK"
   },
   "source": [
    "Bom desenvolvimento!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nze8UbKhosm9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Davidson\\AppData\\Local\\Temp/ipykernel_13060/2353454606.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.*` instead of `tqdm._tqdm_notebook.*`\n",
      "  from tqdm._tqdm_notebook import tqdm_notebook\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas()\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ue0nV0uVo3OZ"
   },
   "outputs": [],
   "source": [
    "# CARREGANDO O DATA FRAME\n",
    "df = pd.read_csv('data/reviews-pt-br.csv', encoding='utf-8')\n",
    "\n",
    "df = df.sample(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FziwgqJmw9OD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2000 entries, 32887 to 5298\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   codigo      2000 non-null   int64 \n",
      " 1   texto       2000 non-null   object\n",
      " 2   sentimento  2000 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 62.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "25cBRwGAw8-1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Davidson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "499"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carrega StopWords\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# stopwords NLTK\n",
    "stops_nltk = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "# stopwords SpaCy\n",
    "stops_spacy = nlp.Defaults.stop_words\n",
    "\n",
    "# stopwords do SpaCy e NLTK combinadas\n",
    "stops = list(set(stops_spacy).union(set(stops_nltk)))\n",
    "len(stops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# função de lematização dos verbos do documento\n",
    "def fn_lematiza_verb_texto(texto):\n",
    "    sent = []\n",
    "    doc = nlp(texto)\n",
    "    for word in doc:\n",
    "        if word.pos_ =='VERB':\n",
    "            sent.append(word.lemma_)\n",
    "        else:\n",
    "            sent.append(word.text)\n",
    "    return \" \".join(sent)\n",
    "\n",
    "# função para limpar documento\n",
    "def fn_limpa_texto(texto):\n",
    "    result = []\n",
    "    texto = re.findall(r\"[a-zA-z]+\", texto)\n",
    "    for w in texto:\n",
    "        w = w.lower()\n",
    "        if w in stops:\n",
    "            continue\n",
    "        result.append(w)\n",
    "        \n",
    "    texto = ' '.join(result)\n",
    "    return texto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18deb3c76c9f431eb9e87b68eedc43d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c805898b8eb4bcbb76386e84e2fb676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccf1876d751f4d5cb8d8ad53ab4fe5af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "# limpa texto\n",
    "df['texto_trat'] = df.texto.progress_apply(fn_limpa_texto)\n",
    "\n",
    "# aplica a lematização dos verbos no dataframe\n",
    "df['texto_lemma_verb'] = df.texto_trat.progress_apply(fn_lematiza_verb_texto)\n",
    "\n",
    "df['sentimento'] = df.sentimento.progress_apply(lambda x: 1 if x == 'pos' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>codigo</th>\n",
       "      <th>texto</th>\n",
       "      <th>sentimento</th>\n",
       "      <th>texto_trat</th>\n",
       "      <th>texto_lemma_verb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32887</th>\n",
       "      <td>32888</td>\n",
       "      <td>outro multid babes bullets hist ria ousada out...</td>\n",
       "      <td>1</td>\n",
       "      <td>outro multid babes bullets hist ria ousada out...</td>\n",
       "      <td>outro multid babes bullets hist rir ousar outr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17716</th>\n",
       "      <td>17717</td>\n",
       "      <td>pena ningu m conhecer pequena obra prima suspe...</td>\n",
       "      <td>1</td>\n",
       "      <td>pena ningu m conhecer pequena obra prima suspe...</td>\n",
       "      <td>pena ningu m conhecer pequena obra prima suspe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8456</th>\n",
       "      <td>8457</td>\n",
       "      <td>haver crian crescido terem visto nica nica bug...</td>\n",
       "      <td>0</td>\n",
       "      <td>haver crian crescido terem visto nica nica bug...</td>\n",
       "      <td>haver crian crescer terem vestir nica nica bug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3051</th>\n",
       "      <td>3052</td>\n",
       "      <td>ok concordaram melhor temporada matar boone m ...</td>\n",
       "      <td>1</td>\n",
       "      <td>ok concordaram melhor temporada matar boone m ...</td>\n",
       "      <td>ok concordar melhor temporada matar boone m de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31202</th>\n",
       "      <td>31203</td>\n",
       "      <td>blockbuster baseketball daqueles filmes costum...</td>\n",
       "      <td>0</td>\n",
       "      <td>blockbuster baseketball daqueles filmes costum...</td>\n",
       "      <td>blockbuster baseketball daqueles filmes costum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14118</th>\n",
       "      <td>14119</td>\n",
       "      <td>classificado vida real londres ent produtores ...</td>\n",
       "      <td>0</td>\n",
       "      <td>classificado vida real londres ent produtores ...</td>\n",
       "      <td>classificado vida real londres ent produtores ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32646</th>\n",
       "      <td>32647</td>\n",
       "      <td>first blood tenta quest tratamento veteranos v...</td>\n",
       "      <td>0</td>\n",
       "      <td>first blood tenta quest tratamento veteranos v...</td>\n",
       "      <td>first blood tentar quest tratamento veteranos ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5227</th>\n",
       "      <td>5228</td>\n",
       "      <td>filme controv rsias escritas projetos verhoeve...</td>\n",
       "      <td>1</td>\n",
       "      <td>filme controv rsias escritas projetos verhoeve...</td>\n",
       "      <td>filme controv rsias escrito projetos verhoeven...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19987</th>\n",
       "      <td>19988</td>\n",
       "      <td>acho maiores filmes burts junto deliverance sm...</td>\n",
       "      <td>1</td>\n",
       "      <td>acho maiores filmes burts junto deliverance sm...</td>\n",
       "      <td>achar maiores filmes burts junto deliverance s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14913</th>\n",
       "      <td>14914</td>\n",
       "      <td>pesadelo insomniacs hist ria homem mergulhar i...</td>\n",
       "      <td>1</td>\n",
       "      <td>pesadelo insomniacs hist ria homem mergulhar i...</td>\n",
       "      <td>pesadelo insomniacs hist rir homem mergulhar i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37457</th>\n",
       "      <td>37458</td>\n",
       "      <td>come arei desculpando cineastas lugares usarem...</td>\n",
       "      <td>0</td>\n",
       "      <td>come arei desculpando cineastas lugares usarem...</td>\n",
       "      <td>comer arar desculpar cineastas lugares usar te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5220</th>\n",
       "      <td>5221</td>\n",
       "      <td>filme veementemente c nico sarc stico intenso ...</td>\n",
       "      <td>1</td>\n",
       "      <td>filme veementemente c nico sarc stico intenso ...</td>\n",
       "      <td>filme veementemente c nico sarc stico intenso ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40097</th>\n",
       "      <td>40098</td>\n",
       "      <td>fat man little boy nomes c digo bombas at mica...</td>\n",
       "      <td>1</td>\n",
       "      <td>fat man little boy nomes c digo bombas at mica...</td>\n",
       "      <td>fat man little boy nomes c dizer bombas at mic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3687</th>\n",
       "      <td>3688</td>\n",
       "      <td>direto n vi filme t s lido dog bite dog f film...</td>\n",
       "      <td>1</td>\n",
       "      <td>direto n vi filme t s lido dog bite dog f film...</td>\n",
       "      <td>direto n vir filme t s lidar dog bite dog f fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4857</th>\n",
       "      <td>4858</td>\n",
       "      <td>voc gosta cagney gostar filme pretens integrid...</td>\n",
       "      <td>1</td>\n",
       "      <td>voc gosta cagney gostar filme pretens integrid...</td>\n",
       "      <td>voc gostar cagney gostar filme pretens integri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39302</th>\n",
       "      <td>39303</td>\n",
       "      <td>dezembro trem sai central estocolmo berlim n r...</td>\n",
       "      <td>1</td>\n",
       "      <td>dezembro trem sai central estocolmo berlim n r...</td>\n",
       "      <td>dezembro trem sair central estocolmo berlim n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19142</th>\n",
       "      <td>19143</td>\n",
       "      <td>expectativas filme produ raj yash m sicas most...</td>\n",
       "      <td>0</td>\n",
       "      <td>expectativas filme produ raj yash m sicas most...</td>\n",
       "      <td>expectativas filme produ raj yash m sicas most...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13595</th>\n",
       "      <td>13596</td>\n",
       "      <td>mim filme pareceu cair rosto principal problem...</td>\n",
       "      <td>0</td>\n",
       "      <td>mim filme pareceu cair rosto principal problem...</td>\n",
       "      <td>mim filme parecer cair rosto principal problem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22691</th>\n",
       "      <td>22692</td>\n",
       "      <td>miriam hopkins dama cabelo ruivo filme biogr f...</td>\n",
       "      <td>1</td>\n",
       "      <td>miriam hopkins dama cabelo ruivo filme biogr f...</td>\n",
       "      <td>miriam hopkins dama cabelo ruivo filme biogr f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31783</th>\n",
       "      <td>31784</td>\n",
       "      <td>ron howard editores s trabalho siga orienta es...</td>\n",
       "      <td>0</td>\n",
       "      <td>ron howard editores s trabalho siga orienta es...</td>\n",
       "      <td>ron howard editores s trabalho seguir orientar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       codigo                                              texto  sentimento  \\\n",
       "32887   32888  outro multid babes bullets hist ria ousada out...           1   \n",
       "17716   17717  pena ningu m conhecer pequena obra prima suspe...           1   \n",
       "8456     8457  haver crian crescido terem visto nica nica bug...           0   \n",
       "3051     3052  ok concordaram melhor temporada matar boone m ...           1   \n",
       "31202   31203  blockbuster baseketball daqueles filmes costum...           0   \n",
       "14118   14119  classificado vida real londres ent produtores ...           0   \n",
       "32646   32647  first blood tenta quest tratamento veteranos v...           0   \n",
       "5227     5228  filme controv rsias escritas projetos verhoeve...           1   \n",
       "19987   19988  acho maiores filmes burts junto deliverance sm...           1   \n",
       "14913   14914  pesadelo insomniacs hist ria homem mergulhar i...           1   \n",
       "37457   37458  come arei desculpando cineastas lugares usarem...           0   \n",
       "5220     5221  filme veementemente c nico sarc stico intenso ...           1   \n",
       "40097   40098  fat man little boy nomes c digo bombas at mica...           1   \n",
       "3687     3688  direto n vi filme t s lido dog bite dog f film...           1   \n",
       "4857     4858  voc gosta cagney gostar filme pretens integrid...           1   \n",
       "39302   39303  dezembro trem sai central estocolmo berlim n r...           1   \n",
       "19142   19143  expectativas filme produ raj yash m sicas most...           0   \n",
       "13595   13596  mim filme pareceu cair rosto principal problem...           0   \n",
       "22691   22692  miriam hopkins dama cabelo ruivo filme biogr f...           1   \n",
       "31783   31784  ron howard editores s trabalho siga orienta es...           0   \n",
       "\n",
       "                                              texto_trat  \\\n",
       "32887  outro multid babes bullets hist ria ousada out...   \n",
       "17716  pena ningu m conhecer pequena obra prima suspe...   \n",
       "8456   haver crian crescido terem visto nica nica bug...   \n",
       "3051   ok concordaram melhor temporada matar boone m ...   \n",
       "31202  blockbuster baseketball daqueles filmes costum...   \n",
       "14118  classificado vida real londres ent produtores ...   \n",
       "32646  first blood tenta quest tratamento veteranos v...   \n",
       "5227   filme controv rsias escritas projetos verhoeve...   \n",
       "19987  acho maiores filmes burts junto deliverance sm...   \n",
       "14913  pesadelo insomniacs hist ria homem mergulhar i...   \n",
       "37457  come arei desculpando cineastas lugares usarem...   \n",
       "5220   filme veementemente c nico sarc stico intenso ...   \n",
       "40097  fat man little boy nomes c digo bombas at mica...   \n",
       "3687   direto n vi filme t s lido dog bite dog f film...   \n",
       "4857   voc gosta cagney gostar filme pretens integrid...   \n",
       "39302  dezembro trem sai central estocolmo berlim n r...   \n",
       "19142  expectativas filme produ raj yash m sicas most...   \n",
       "13595  mim filme pareceu cair rosto principal problem...   \n",
       "22691  miriam hopkins dama cabelo ruivo filme biogr f...   \n",
       "31783  ron howard editores s trabalho siga orienta es...   \n",
       "\n",
       "                                        texto_lemma_verb  \n",
       "32887  outro multid babes bullets hist rir ousar outr...  \n",
       "17716  pena ningu m conhecer pequena obra prima suspe...  \n",
       "8456   haver crian crescer terem vestir nica nica bug...  \n",
       "3051   ok concordar melhor temporada matar boone m de...  \n",
       "31202  blockbuster baseketball daqueles filmes costum...  \n",
       "14118  classificado vida real londres ent produtores ...  \n",
       "32646  first blood tentar quest tratamento veteranos ...  \n",
       "5227   filme controv rsias escrito projetos verhoeven...  \n",
       "19987  achar maiores filmes burts junto deliverance s...  \n",
       "14913  pesadelo insomniacs hist rir homem mergulhar i...  \n",
       "37457  comer arar desculpar cineastas lugares usar te...  \n",
       "5220   filme veementemente c nico sarc stico intenso ...  \n",
       "40097  fat man little boy nomes c dizer bombas at mic...  \n",
       "3687   direto n vir filme t s lidar dog bite dog f fi...  \n",
       "4857   voc gostar cagney gostar filme pretens integri...  \n",
       "39302  dezembro trem sair central estocolmo berlim n ...  \n",
       "19142  expectativas filme produ raj yash m sicas most...  \n",
       "13595  mim filme parecer cair rosto principal problem...  \n",
       "22691  miriam hopkins dama cabelo ruivo filme biogr f...  \n",
       "31783  ron howard editores s trabalho seguir orientar...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pré-processamento vetorização e BOW\n",
    "# Frequência de termos - term frequency–inverse document frequency (TF-IDF)\n",
    "vetor = TfidfVectorizer(ngram_range=(1,2), use_idf=True) #0.7729036810537457 - TF-IDF, texto lematixado, uni e bi brama, Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# texto tratado e verbos lematizado\n",
    "vetor.fit(df.texto_lemma_verb)\n",
    "text_vect = vetor.transform(df.texto_lemma_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 187988)\n"
     ]
    }
   ],
   "source": [
    "# separa as amostras de Treino (80%) e Texte (20%)\n",
    "X_train,X_test,y_train,y_test = train_test_split(text_vect, df['sentimento'], test_size = 0.2, random_state = 42)\n",
    "print(text_vect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define e treina o modelo de Logistic Regression\n",
    "modelo_lr = LogisticRegression(random_state=42)\n",
    "\n",
    "modelo_lr.fit(X_train, y_train)\n",
    "\n",
    "# valida a parformance\n",
    "y_prediction = modelo_lr.predict(X_test)\n",
    "logistic_regression_accuracy = accuracy_score(y_prediction, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define e treina o modelo de Logistic Regression com penalty\n",
    "modelo_lrp = LogisticRegression(penalty = 'l2', C = 100, random_state=42)\n",
    "\n",
    "modelo_lrp.fit(X_train, y_train)\n",
    "\n",
    "# valida a parformance\n",
    "y_prediction = modelo_lrp.predict(X_test)\n",
    "logistic_regression_penalty_accuracy = f1_score(y_prediction, y_test, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define e treina o modelo de classificação Decision Tree Classifier\n",
    "modelo_dtc = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "modelo_dtc.fit(X_train, y_train)\n",
    "\n",
    "# valida a parformance\n",
    "y_prediction = modelo_dtc.predict(X_test)\n",
    "decision_tree_accuracy = f1_score(y_prediction, y_test, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define e treina o modelo de Multinomial\n",
    "modelo_m = MultinomialNB()\n",
    "\n",
    "modelo_m.fit(X_train, y_train)\n",
    "\n",
    "# valida a parformance\n",
    "y_prediction = modelo_m.predict(X_test)\n",
    "multinomial_accuracy = f1_score(y_prediction, y_test, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic_regression_accuracy=0.805\n",
      "logistic_regression_penalty_accuracy=0.8232866211522338\n",
      "decision_tree_accuracy=0.6474007197621655\n",
      "multinomial_accuracy=0.7919006913890635\n"
     ]
    }
   ],
   "source": [
    "print(f'{logistic_regression_accuracy=}')\n",
    "print(f'{logistic_regression_penalty_accuracy=}')\n",
    "print(f'{decision_tree_accuracy=}')\n",
    "print(f'{multinomial_accuracy=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68SiMjcWqD_m"
   },
   "source": [
    "####**Validação do professor**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T24EasckqG2I"
   },
   "source": [
    "Consolidar apenas os scripts do seu **modelo campeão**, desde o carregamento do dataframe, separação das amostras, tratamentos utilizados (funções, limpezas, etc.), criação dos objetos de vetorização dos textos e modelo treinado e outras implementações utilizadas no processo de desenvolvimento do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZxqHA-XCrqsD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VFA-CYfawkEJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BuJtvcfXo3J4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ULYNH6-o3Hf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ClM-JTJo3FK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TemplateTrabalhoFinal-NLP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
